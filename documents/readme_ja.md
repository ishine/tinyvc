# TinyVC : Towards AI voice conversion on CPU
![](../images/tinyvc_logo.png)
(このリポジトリは実験段階のものです。内容は予告なく変更される場合があります。)

## 学習済みモデルをダウンロード
[学習済みモデル](https://huggingface.co/uthree/tinyvc/tree/main)をダウンロードすることで、すぐにこのボイスチェンジャを試すことができます。

## モデル構造
![](../images/tinyvc_architecture.png)

## 特徴
- リアルタイム変換
- 低遅延 (約0.2秒程度、環境や最適化によって変化する可能性あり。)
- 位相とピッチが安定している (ソースフィルタモデルに基づく。)
- k近傍法による話者スタイル変換
- 加算シンセサイザによる完全にF0制御可能な音声合成

## 必要なもの
- Python 3.10 以降
- PyTorch 2.0以降と GPU 環境
- フルスクラッチで訓練する場合は多数の人間の音声データを用意すること。(LJ SpeechやJVS コーパスなど。)

## インストール
1. このリポジトリをクローン
```sh
git clone https://github.com/uthree/tinyvc.git
```
2. 依存関係をインストール
```sh
pip3 install -r requirements.txt
```

## 事前学習
基礎的な音声変換を行うモデルを学習する。この段階では特定の話者に特化したモデルになるわけではないが、基本的な音声合成ができるモデルをあらかじめ用意しておくことで、少しの調整だけで特定の話者に特化したモデルを学習することができる。

1. 前処理
音声ファイルがたくさん入ったディレクトリを用意して以下のコマンドを実行
```sh
python3 preprocess.py <データセットのディレクトリ>
```

2. エンコーダーを学習。  
HuBERT, ピッチ推定を蒸留する。
```sh
python3 train_encoder.py
```

3. デコーダーを学習  
デコーダーは、ピッチとコンテンツから元の波形を再構築することを目標とする。

```sh
python3 train_decoder.py
```

## ファインチューニング
事前学習したモデルを、特定話者への変換に特化したモデルに調整することによって、より精度の高いモデルを製作することが可能です。この工程は事前学習と比べて非常に少ない時間で完了します。
1. 特定話者の音声ファイルのみを一つのフォルダにまとめ、前処理をする。
```sh
python3 preprocess.py
```

2. デコーダーをファインチューニングする。
```sh
python3 train_decoder.py
```
3. ベクトル検索用の辞書を作成する。これにより毎回音声ファイルをエンコードする必要がなくなります。
```sh
python3 extract_index.py -o <辞書の出力先(任意)>
```
4. 推論する際は`-idx <辞書ファイル>`オプションをつけることで任意の辞書データを読み込むことができます。
デフォルトの辞書ファイルの出力先は `models/index.pt`。

### 学習オプション
- `-fp16 True` をつけると16ビット浮動小数点数による学習が可能。RTXシリーズのGPUの場合のみ可能。
- `-b <number>` でバッチサイズを変更。デフォルトは `16`。
- `-e <number>` でエポック数を変更。 デフォルトは `60`。
- `-d <device name>` で演算デバイスを変更。 デフォルトは`cuda`。

## 推論
1. `inputs` フォルダを作成する。
2. `inputs` フォルダに変換したい音声ファイルを入れる
3. 推論スクリプトを実行する
```sh
python3 infer.py -t <ターゲットの音声ファイル>
```
また、辞書ファイルを利用する場合は
```sh
python3 infer.py -idx <辞書ファイル>
```

### 追加のオプション
- `-d <デバイス名>` で演算デバイスを変更できます。もともと高速なのであまり意味がないかもしれませんが。
- `-p <音階>` でピッチシフトを行うことができます。男女間の音声変換に有用です。12で1オクターブ上げます。
- `--no-chunking True` で品質を上げることができますが、多くのRAMを必要とします。

## pyaudioによるリアルタイム推論 (テスト段階の機能です)
1. オーディオデバイスのIDを確認
```sh
python3 audio_device_list.py
```

2. 実行
```sh
python3 infer_streaming.py -i <入力デバイスID> -o <出力デバイスID> -l <ループバックデバイスID> -t <ターゲットの音声ファイル>
```
(ループバックのオプションはつけなくても動作します。)

## 参考文献
- [FastSVC](https://arxiv.org/abs/2011.05731)
- [kNN-VC](https://arxiv.org/abs/2305.18975)
- [WavLM](https://arxiv.org/pdf/2110.13900.pdf) (Fig. 2)
- [StreamVC](https://arxiv.org/abs/2401.03078v1)
- [Hifi-GAN](https://arxiv.org/abs/2010.05646)
- [AdaIN](https://arxiv.org/abs/1703.06868)
